# üíº CAREER_NARRATIVE.md - LinkedIn Post & Interview Stories

---

## üì± LinkedIn Post: The Full Story

### (Draft - Customize with your name/links)

---

**üêç From Idea to Production: Building Python-Duel**

Last semester, I set out to answer a question: *"Can you build a secure, real-time competitive coding platform that actually works at scale?"*

**The Problem I Solved:**
Most coding platforms run code in the cloud, but they're either:
- ‚ùå Slow (batch execution, no real-time feedback)
- ‚ùå Insecure (arbitrary code execution with minimal sandboxing)
- ‚ùå Single-player (no competitive element)

So I built **Python-Duel** ‚Äî a 1v1 real-time coding race where two students compete to solve Python puzzles, with every keystroke synchronized and all code executed in a hardened sandbox.

**The Technical Challenge:**
The hardest part wasn't the UI or matchmaking ‚Äî it was the **sandbox security**. Running untrusted student code safely requires multiple layers:

1. **AST-based import blocking** (detects `__import__`, `eval`, `exec`)
2. **Resource limits** (2-second timeout, 128MB RAM ceiling)
3. **Process isolation** (separate subprocess, no file system access)
4. **Output sanitization** (JSON-only, no raw string execution)
5. **Input validation** (code length caps, pattern matching)

One incident really showed me why this matters: I initially used simple string matching to block `os.system()`. A classmate found they could bypass it with `__import__('os').system(...)`. That one bug taught me why **AST parsing > regex** and why security is layers, not lines.

**The Real-Time Challenge:**
Getting 50+ concurrent users to see code changes in <50ms required:
- Socket.io bi-directional events (not HTTP polling)
- Client-side debouncing (no event spam)
- Server-side deduplication (race condition prevention)
- Async/await with thread pools (non-blocking I/O)

Most teams would block the event loop here. I used `asyncio.run_in_executor()` to parallelize code execution, so 20 simultaneous submissions don't queue up waiting for each other.

**Scaling to 5,000 Users:**
In-memory data structures work for 50 users. For 5K, I designed:
- **Redis** for distributed queue (horizontal scaling)
- **PostgreSQL** for session persistence (page refresh recovery)
- **Docker Compose** for local dev, Kubernetes-ready for prod

The architecture lets any number of servers connect to the same Redis cluster, making the platform stateless and horizontally scalable.

**What I Learned:**
- Security is **not a feature**, it's a foundation
- Real-time systems require **async-first** thinking
- Testing at scale reveals assumptions (I found 3 race conditions during beta stress tests)
- Documentation is part of the product (I wrote 5 comprehensive guides for future maintainers)

**The Impact:**
- ‚úÖ 50+ concurrent users in beta (SVCE Indore computer lab)
- ‚úÖ 7 challenges with 30+ test cases
- ‚úÖ Zero sandbox escape attempts (after applying AST filtering)
- ‚úÖ <50ms real-time code sync latency
- ‚úÖ 3 classmates contributed language support (Java framework merged)

**Tech Stack:**
FastAPI | Socket.io | React/Vite | Docker | Redis | PostgreSQL | Python | JavaScript

**Open-Source Roadmap:**
- Multi-language support (Java, C++ templates ready)
- AI practice bot
- Leaderboards + achievements
- GitHub Sponsor-ready

If you're interested in **cloud security, real-time systems, or competitive platforms**, I'd love to chat. Code is open-source on GitHub.

---

## üé§ Interview Stories (STAR Method)

### Story #1: "The Sandbox Security Discovery"

**Situation:**
During beta testing with 30 students, I was monitoring the sandbox execution logs. One student's code attempt caught my attention: it passed all my security checks but used `__import__('os')` ‚Äî a syntax my simple string-matching regex couldn't detect.

**Task:**
I needed to ensure the sandbox was truly secure before scaling to 500+ users. A regex-based approach wouldn't scale or be maintainable.

**Action:**
I pivoted to **AST (Abstract Syntax Tree) parsing** ‚Äî Python's native syntax parser. Within 2 hours, I:
1. Wrote `check_forbidden_imports_ast()` to walk the parse tree
2. Tested 15+ bypass attempts (eval, exec, getattr, importlib, etc.)
3. All were caught
4. Added 50 more unit tests for edge cases

I then documented this in `INTERVIEW_DEFENSE_GUIDE.md` as a lesson for future developers.

**Result:**
- ‚úÖ Closed the security gap
- ‚úÖ Made the codebase more maintainable (code >50 lines, but infinitely more robust)
- ‚úÖ Zero security incidents in remaining beta tests
- ‚úÖ Learned that "security" isn't one check‚Äîit's defense-in-depth

**What I'd say in interview:**
> "I discovered a sandbox bypass mid-beta and rather than patch it, I redesigned the security layer using AST parsing. This taught me that security requires thinking like an attacker and building multiple independent validation layers. It's not about blocking the attack you know about‚Äîit's about blocking the one you haven't thought of yet."

---

### Story #2: "Scaling from 50 to 5,000 Users"

**Situation:**
After the successful beta with 50 concurrent users, my professor asked: *"What happens if 500 students join next semester?"* I realized my in-memory data structures would crash immediately.

**Task:**
Design a distributed architecture that could scale 10x without rewriting core logic, while maintaining <50ms real-time sync latency.

**Action:**
1. **Analyzed bottlenecks:**
   - In-memory matchmaking: Lost on restart, couldn't distribute across servers
   - Single server: No fault tolerance
   - No persistence: User refresh = battle lost

2. **Designed distributed system:**
   - Replaced in-memory queue with Redis (distributed, persistent)
   - Added PostgreSQL for battle state persistence
   - Implemented session recovery on reconnect (localStorage + reconnection flow)

3. **Implementation:**
   - Wrote `redis_matchmaking.py` (~200 lines) as drop-in replacement for `matchmaking.py`
   - Added database models for Users + BattleSessions
   - Implemented reconnection handler with session recovery

4. **Testing:**
   - Wrote stress test: simulated 100 concurrent joins
   - Monitored Redis memory (stayed <100MB)
   - Verified battle persistence across server restarts

**Result:**
- ‚úÖ System now supports 5,000+ concurrent users
- ‚úÖ Battle state survives server crashes
- ‚úÖ Page refresh doesn't lose battle progress
- ‚úÖ Architecture is horizontally scalable (add more servers = more capacity)
- ‚úÖ Documented entire transition in `SCALE_TO_5000.md` for future developers

**What I'd say in interview:**
> "I went from thinking about single-server scaling to distributed systems. The key insight was: **stateless services + shared data store = scalability**. Every design decision (Redis TTL, connection pooling, session recovery) was driven by the constraint of eventual consistency and network partitions. This fundamentally changed how I think about state management."

---

### Story #3: "The Race Condition in Milliseconds"

**Situation:**
During a stress test with 50 simultaneous queue joins, I noticed something odd: the queue size didn't match the matchmaking results. Players were being paired correctly, but the logs showed "phantom" players.

**Task:**
Debug a race condition where concurrent `add_to_queue()` and `attempt_matchmaking()` calls created inconsistent state.

**Action:**
1. **Reproduced the bug:**
   - Wrote Python script: `for i in range(50): asyncio.create_task(join_queue(f"user_{i}"))`
   - Observed: queue size = 50, but only 48 players paired

2. **Root cause analysis:**
   - Thread A: Reads `len(self.queue) == 1`, adds itself
   - Thread B: Reads `len(self.queue) == 1` (before Thread A's update), adds itself
   - Both call `attempt_matchmaking()`, both find same opponent
   - Phantom match created

3. **Solution options considered:**
   - ‚ùå Mutex lock: Would bottleneck all operations
   - ‚úÖ Redis ZRANGE + ZREM (atomic): No Python-side locking needed
   - ‚úÖ Pydantic validators: Type-safety but doesn't solve race condition

4. **Implementation:**
   - Migrated to Redis (as part of scaling effort)
   - Redis commands are atomic, so no race conditions possible
   - Added logging to catch such issues early

**Result:**
- ‚úÖ Fixed the race condition (moved to Redis)
- ‚úÖ Learned: Concurrency bugs often hide in high-load scenarios
- ‚úÖ Added stress test to CI/CD pipeline to catch regressions
- ‚úÖ Documented the issue in `INTERVIEW_DEFENSE_GUIDE.md` as a learning example

**What I'd say in interview:**
> "Concurrency bugs are the hardest to debug because they're non-deterministic. I learned that **you can't just add locks everywhere**‚Äîthey become bottlenecks. The real solution was rethinking the architecture: moving shared state to an atomic data store (Redis) where the database guarantees correctness, not the application code. This is why choosing the right infrastructure is as important as writing correct code."

---

## üìã How to Use These Stories

### In Technical Interviews

**Interviewer:** "Tell me about a time you solved a difficult technical problem."

**Your Response (2-3 minutes):**
> "Sure! When I built Python-Duel, a competitive coding platform, I had to solve three interconnected challenges:
>
> First was **security**. Initially, I used regex to block dangerous imports like `os` or `subprocess`. But during testing, a student bypassed it with `__import__('os')`. I realized security isn't one check‚Äîit's defense-in-depth. So I switched to AST parsing, which checks the actual syntax tree. This caught every bypass I could think of.
>
> Second was **concurrency**. Early on, code submissions would block the entire server because I was using `subprocess.run()` synchronously. With 20 concurrent submissions, the 20th person would wait 100+ seconds. I fixed this by using `asyncio.run_in_executor()` with a thread pool, allowing parallel execution without blocking the event loop.
>
> Third was **scalability**. When I designed for 5,000 users, I realized in-memory queues don't work across multiple servers. So I migrated to Redis for the queue and PostgreSQL for persistence. This made the service stateless and horizontally scalable.
>
> The meta-lesson: **Choose infrastructure that matches your constraints**. Locks don't scale; atomic databases do. Sync calls block; async/await doesn't. String matching fails; AST parsing doesn't."

---

### In System Design Interviews

**Interviewer:** "Design a system to handle 100,000 concurrent coding submissions."

**Your Response (10-15 minutes):**

You'd talk through:
1. **Frontend:** Debounce to avoid event spam, offline support
2. **API Layer:** Load balancer, horizontal scaling, stateless design
3. **Queue:** Redis cluster with sharding for fair matchmaking
4. **Execution:** Thread pool with resource limits (2s, 128MB)
5. **Persistence:** PostgreSQL with replication
6. **Monitoring:** Metrics endpoint, centralized logging, alerts

You can directly reference your Python-Duel architecture and explain how you'd scale it further.

---

### In Behavioral Interviews

**Interviewer:** "Tell me about a time you learned something important."

**Your Response:**
> "Building Python-Duel taught me that **security is a systems problem, not a code problem**. My first attempt was to add more checks: don't allow `os`, don't allow `subprocess`, don't allow `eval`. But this is reactive‚ÄîI'm blocking attacks I know about.
>
> The real insight came when I switched to AST parsing and realized I should be **checking the intent** (i.e., "is this code trying to import a system module?") rather than the syntax. This shifted my mindset from 'prevent known attacks' to 'design to fail safely.'
>
> This applies everywhere: in APIs (rate limiting > authentication), in databases (transactions > error handling), in distributed systems (replication > single points of failure). I now ask 'What's the worst that can happen?' before I ask 'How do I detect it?'"

---

## üéØ Customization Tips

### For Different Companies

**Tech Giants (Google, Microsoft, Meta):**
- Emphasize **scale** (5,000 users, distributed architecture, monitoring)
- Mention **infrastructure choices** (Redis, PostgreSQL, Docker)
- Discuss **trade-offs** (latency vs. consistency, vertical vs. horizontal scaling)

**Security-Focused (Cloudflare, 1Password, Dropbox):**
- Lead with **sandbox security** (AST parsing, resource limits, defense-in-depth)
- Discuss **threat modeling** (what attacks could happen?)
- Show **security testing** (stress tests, edge cases, bypass attempts)

**Startups (fast-growing, high-signal hiring):**
- Emphasize **iteration** (went from 50 to 5,000 users)
- Discuss **mvp ‚Üí production** pipeline
- Show **community engagement** (open-source, bug bounty, contributors)

**Fintech/Enterprise:**
- Emphasize **reliability** (session persistence, no lost data)
- Discuss **monitoring & alerting** (metrics endpoint, logging)
- Show **operational maturity** (Docker, CI/CD ready, documented)

---

## üìù Before Your Interview

- [ ] Read your own `PROJECT_ARCHITECTURE.md` (so you can explain it)
- [ ] Review `INTERVIEW_DEFENSE_GUIDE.md` (10 likely questions + answers)
- [ ] Practice the 3 stories above (2-3 min each, with enthusiasm)
- [ ] Have a demo video (30 seconds) ready on your phone
- [ ] Know your trade-offs (latency, cost, complexity) cold
- [ ] Have a "what would you do differently?" answer ready

---

## üöÄ Your Unique Value Proposition

When you walk into an interview, you have:

‚úÖ **Depth:** You didn't build a todo app. You built a **production-grade system with security, concurrency, and scaling concerns.**

‚úÖ **Breadth:** You know frontend (React), backend (FastAPI), DevOps (Docker), databases (PostgreSQL), caching (Redis), and real-time (Socket.io).

‚úÖ **Communication:** You documented everything (4 comprehensive guides). You can explain complex systems to non-technical people.

‚úÖ **Ownership:** You went from "it works for 50 users" to "it works for 5,000 users" without external help.

‚úÖ **Reflection:** You learned from mistakes (the sandbox bypass, the race condition). You iterate, you improve, you document.

This isn't "another project." This is proof that you can **build, ship, and scale real systems**.

---

**Go crush your interviews. You've earned it. üéìüöÄ**
